\chapter{Statistics: starter guide}
\section{Basic objects}
    \subsection{Statistics}
        \begin{definition}
            Let P be a probability measure on $\mathbb{R}^d$. Let $n\geq1$. A \important{size n sample} of law P is an i.i.d. sequence $X_1, \ldots, X_n$ of random variables/vectors having law P. A \important{realisation} of a size n sample is a sequence of values of the random variables: that is a sequence $\left(x_1, \ldots, x_n\right) \in \left(\mathbb{R}^d\right)^n$.
        \end{definition}
        \begin{definition}
            A \important{statistic} on a size n sample $X_1, \ldots, X_n$ with values in $\mathbb{R}^d$ is a random variable $Y:\left(\mathbb{R}^d\right)^n \to \mathbb{R}$.
        \end{definition}
    \subsection{Estimators}
        \begin{definition}
            We are interested in \important{parametric estimation}. We will denote $\mathbb{P}_{\theta}$ the law that we are trying to approximate, $\mathbb{P}_{\theta}$ is thus a probability measure on $\mathbb{R}^d$. We will suppose that it is entirely determined by a parameter $\theta \in \Theta$, where $\Theta \subset \mathbb{R}^m$ for some $ m\geq1$ is the \important{parameter space}.
        \end{definition}
        \begin{remark}{Examples}
            \begin{tcolorbox}[gris]
                We could for example look at 
                \begin{enumerate}[left=10pt]
                    \item a squence of height measurements in the Swiss population, 
                    \item a sequence of life duration for computers in a company ,
                    \item the number of blue cars exiting the highway at Ecublens during a day.
                \end{enumerate}
                We can now chose an apprriate family of probability measures as candidates for approximating each of these situations.
                \begin{enumerate}[left=10pt]
                    \item One can use a Gaussian law $\mathbb{P}_{\theta} = \mathcal{N}\left(\mu, \sigma ^2\right)$. The parameter defining the law is $\theta = \left(\mu, \sigma\right)$. It leads to the parameter space $\Theta = \mathbb{R} x \left[0, +\infty\right).$ 
                    \item One can use an exponential law $\mathbb{P}_{\theta} = \text{Exp}\left(\lambda\right)$. The parameter is then $\theta = \lambda$, and the parameter space is $\Theta = \left(0, +\infty\right)$. 
                    \item One can use a Poisson law $\mathbb{P}_{\theta}=\text{Poi}\left(\lambda\right)$. The parameter is then $\theta = \lambda$, and the parameter space is $\Theta = \left[0, +\infty\right)$.
                \end{enumerate}
            \end{tcolorbox}
        \end{remark}
        \begin{definition}
            Let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$. For $f : \Theta \to \mathbb{R}$, an \important{estimator} of $f\left(\theta\right)$ is a statistic $\hat{f}:\left(\mathbb{R}^d\right)^n \to f\left(\Theta\right)$, which does not depend on the parameter $\theta$. In words, an estimator is a way to probe the parameter space without prior knowledge of the parameter value to approximate the parameter defining our probability law $\mathbb{P}_{\theta}$.
        \end{definition}
        The first notion makes precise the idea that as we increase a sample size, we can find better and better approximations of our parameter.
        \begin{definition}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. We say that the sequence of estimators $\left(\hat{f}_n\right)_{n >=1}$ is \important{convergent} if $\hat{f}_n$ converges in probability towards $f\left(\theta\right): $ for any $\epsilon >0$ and for any $\theta \in \Theta$, 
            \[\lim_{n \to \infty} P\left(\left|\hat{f}_n\left(X_1, \ldots, X_n\right)-f\left(\theta\right)\right| \geq \epsilon\right)=0\]
        \end{definition}
        The next notions ar formalizations of the estimator gives the correct value on average.
        \begin{definition}
            Let $X_1, \ldots, X_n$ be an n-sample of law $\mathbb{P}_{\theta}$. Let $\hat{f}$ be an estimator of $f\left(\theta\right)$. The \important{bias of }$\hat{f}$ is the difference between the expected value of $\hat{f}$ and $f\left(\theta\right)$: 
            \[\text{Bias}_{\theta}\left(\hat{f}\right)=E\left(\hat{f}\left(X_1, \ldots, X_n\right)\right)-f\left(\theta\right).\]
            $\hat{f}$ is called \important{unbiased} if for any $\theta \in \Theta$, $\text{Bias}_{\theta}\left(\hat{f}\right)=0$. Otherwise, it is called \important{biased}.
        \end{definition}
        \begin{definition}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. We say that the sequence of estimators $\left(\hat{f}_n\right)_{n \geq1}$ is \important{asymptotically unbiased} if for any $\theta \in \Theta$, 
            \[\lim_{n \to \infty} \text{Bias}_{\theta}\left(\hat{f}_n\right)=0.\]
        \end{definition}
        \begin{definition}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. The \important{variance} of $\hat{f}_n$, denoted $\text{Var}_{\theta}\left(\hat{f}_n\right)$, is simply the variance of the random variable $\hat{f}_n\left(X_1, \ldots, X_n\right)$: 
            \[\text{Var}_{\theta}\left(\hat{f}_n\right)=\text{Var}\left(\hat{f}_n\left(X_1, \ldots, X_n\right)\right).\]
        \end{definition}
        We can give a first example of criterion guaranteeing that a sequence of estimators converges.
        \begin{theoreme}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. Suppose that
            \begin{enumerate}[left=10pt]
                \item $\hat{f}_n$ is asymptotically unbiased, 
                \item Var$\left(\hat{f}_n\right) \xrightarrow{n \to \infty} 0.$
            \end{enumerate}
            Then, $\hat{f}_n$ is a convergent sequence of estimators.
        \end{theoreme}
    \subsection{Some examples of estimators}
        \subsubsection{Empirical mean}
            The empirical mean is the estimator we already encountered several times. It is an estimator for the expected value and it is given by 
            \[\bar{X}_n = \frac{1}{n}\sum_{ i=1 } ^{ n }X_i.\]
            It is so widelyused that is got its own standard notation. It is diret to see that this estimator is unbiased: 
            \[E\left(\frac{1}{n}\sum_{ i=1 } ^{ n }X_i\right)=\frac{1}{n}\sum_{ i=1 } ^{ n }E\left(X_i\right)=E\left(X_1\right),\]
            as the $X_i$'s are identically distributed. Moreover, the weak Law of Large Numbers implies that it is a convergent estimator as soon as $E\left(X_1\right)$ is well defined.        
        \subsubsection{Empirical median}
            A median for a random variable X is a number M such that 
            \[P\left(X\leqM\right)=P\left(X\geq M\right)=\frac{1}{2}.\]
            The empirical median is obtained as follows.
            \begin{enumerate}[left=10pt]
                \item Order the sample $X_1, \ldots, X_n$ in a non-decreasing fashion: let $\left(\widetilde{X}_1, \ldots, \widetilde{X}_n\right)$ be a permutation of $X_1, \ldots, X_n$ such that $\widetilde{X}_{i+1}\geq \widetilde{X}_i$ for all i's.
                \item Define the empirical median of $X_1, \ldots, X_n$ to be 
                \[
                \begin{functionbypart}{}
                    \widetilde{X}_{\frac{n+1}{2}} \mathspace \mathspace \mathspace \mathspace \text{ if } n \text{ is odd,}  \\
                    \frac{1}{2}\left(\widetilde{X}_{\frac{n}{2}}+\widetilde{X}_{1+\frac{n}{2}}\right) \mathspace \text{ if } n \text{ is even.}
                \end{functionbypart}
            \]
            \end{enumerate}
            One can show that when the law $\mathbb{P}_{\theta}$ is symmetric around its mean (if it is a continuous law with density $f_{\theta}$, this means $f_{\theta}\left(\mu+x\right)=f_{\theta}\left(\mu-x\right)$ with $\mu=\int_{-\infty}^{+\infty}xf_{\theta}\left(x\right)dx$, the median is an unbiased estimator of the mean.
        \subsubsection{Empirical variance}
            We already have an estimator for the expectation: $\bar{X}_n$. A natural thing to do is to define the empirical variance as 
            \[\sigma_n ^2=\frac{1}{n}\sum_{ i=1 } ^{ n }\left(X_i-\bar{X}_n\right)^2=\frac{1}{n}\sum_{ i=1 } ^{ n }X_i ^2-\bar{X}^2_n=\bar{X}^2_n-\bar{X}^2_n.\]
            I.e: take the difference between the empirical mean of $X_1^2, \ldots, X^2_n$ and the square of the empirical mean of $X_1, \ldots, X_n$.
        \subsubsection{Empirical covariance}
            Consider an n-sample $\left(X_1, Y_1\right), \ldots, \left(X_n, Y_n\right)$ of random vectors of law $\mathbb{P}_{\theta}$. The goal is to estimate the covariance $\text{Cov}\left(X,Y\right)$ where $\left(X,Y\right) \sim \mathbb{P}_{\theta}$. Define the estimator 
            \[\hat{\tau}_n\left(\left(X_1, Y_1\right), \ldots, \left(X_n, Y_n\right)\right)=\frac{1}{n}\sum_{ i=1 } ^{ n }X_iY_i-\frac{1}{n}\left(\sum_{ i=1 } ^{ n }X_i\right)\frac{1}{n}\left(\sum_{ i=1 } ^{ n }Y_i\right)=\bar{XY}_n - \bar{X}_n\bar{Y}_n.\]
\section{Constructing estimators}
    \subsection{Moments method}
        Suppose one wants to estimate a parameter $\theta$ from a sample $X_1, \ldots, X_n$ of law $\mathbb{P}_{\theta}$. The general idea is 
        \begin{enumerate}[left=10pt]
            \item find functions h,g such that we have the relation 
            \[\theta = h\left(E\left(g\left(X\right)\right)\right)\]
            with $X \sim \mathbb{P}_{\theta}$; 
            \item use the empirical mean $\frac{1}{n}\sum_{ i=1 } ^{ n }g\left(X_i\right)$ to estimate $E\left(g\left(X\right)\right)$; 
            \item use the estimator of $\theta$ given by 
            \[\hat{\theta}_n = h\left(\frac{1}{n}\sum_{ i=1 } ^{ n }g\left(X_i\right)\right).\]
        \end{enumerate}
        \begin{remark}{Example}
            \begin{tcolorbox}[gris]
                We can look at the case of $\mathbb{P}_{\theta}=\text{Geo}\left(p\right)\left(\text{ so that } \theta =p, \Theta = \left[0,1\right]\right)$.

                We know that if $X \sim \text{Geo}\left(p\right)$, 
                \[E\left(X\right)=\frac{1}{p}, \mathspace E\left(X^2\right)=\frac{2-p}{p^2}.\]
                Each of these give rise to estimator via the moment method.
                \begin{itemize}[left=10pt, label=\textbullet]
                    \item First consider the first moment. We can use $h\left(x\right)=x^{-1}$ and $g\left(x\right)=x$. This gives the estimator of p 
                    \[\hat{p}_n\left(X_1, \ldots, X_n\right)=\frac{n}{\sum_{ i=1 } ^{ n }X_i}.\]
                    
                    \item Then consider the second moment. We can use $h\left(x\right)=\frac{\sqrt{1+8x}-1}{2x}$ and $g\left(x\right)=x^2$.

                        This gives the estimator of p 
                        \[\hat{p}_n\left(X_1, \ldots, X_n\right)=\frac{n}{2 \sum_{ i=1 } ^{ n }X_i^2}\left(\left(1+\frac{8}{n}\sum_{ i=1 } ^{ n }X_i^2\right)^{\frac{1}{2}}-1\right).\]
                        
                \end{itemize}
            \end{tcolorbox}
        \end{remark}
    \subsection{Maximum likelihood estimator}
        This estimator is deeply linked to the Bayes formula: imagine that you have a realisation $x_1, \ldots, x_n$ of a sample $X_1, \ldots, X_n$ of law $\mathbb{P}_{\theta}$. The idea is to look for the value of $\theta$ which maximises the probability to observe the realisation $x_1, \ldots, x_n$.
        \begin{definition}
            Let $n >=1$, and a sample $X_1, \ldots, X_n$ of law $\mathbb{P}_{\theta}$. For a realisation $x_1, \ldots, x_n$ of $X_1, \ldots, X_n$, the \important{likelihood} of $x_1, \ldots, x_n$ given $\theta, \mathspace\mathcal{L}\left(x_1, \ldots, x_n | \theta\right)$, is defined by 
            \begin{itemize}[left=10pt, label=\textbullet]
                \item if $\mathbb{P}_{\theta}$ is a discrete probability measure (i.e.: the $X_i$'s are discrete random variables), 
                    \[\mathcal{L}\left(x_1, \ldots, x_n|\theta\right)=P\left(X_1=x_1, \ldots, X_n = x_n\right)= \prod_{i=1}^{ n } P\left(X_i =x_i\right)=\prod_{i=1}^{ n } \mathbb{P}_{\theta}\left(\{x_i\}\right)\]
                as the $X_i$'s are independent and of law $\mathbb{P}_{\theta}$; 
                \item if $\mathbb{P}_{\theta}$ is a continuous probability measure (i.e.: the $X_i$'s are continuous random variables), 
                \[\mathcal{L}\left(x_1, \ldots, x_n|\theta\right)=\prod_{i=1}^{ n } f_{X_i}\left(x_i\right)=\prod_{i=1}^{ n } f_{\theta}\left(x_i\right)\]
                where $f_{\theta}$ is the density function associated to $\mathbb{P}_{\theta}$.
            \end{itemize}
        \end{definition}
        \begin{definition}
            The \important{maximum likelihood estimator} of $\theta$ is given by 
        \[MLE\left(X_1, \ldots, X_n\right) =argmax_{\theta}\mathcal{L}\left(X_1, \ldots, X_n|\theta\right),\]
            the point $\theta$ at which $\mathcal{L}\left(X_1, \ldots, X_n|\theta\right)$ is maximized.
        \end{definition}
        \begin{remark}{Example}
            \begin{tcolorbox}[gris]
                Consider the case $X_1, \ldots, X_n$ is a n-sample of law $\mathcal{N}\left(\mu, \sigma ^2\right)$ (and take $\theta=\left(\mu, \sigma ^2\right)$). The likelihood function of a realisation $x_1, \ldots, x_n \in \mathbb{N}^*$ is given by 
                \[\mathcal{L}\left(x_1, \ldots, x_n| \mu, \sigma ^2\right) = \prod_{i=1}^{ n } \frac{e^{-\left(x_i-\mu\right)^2 \slash 2\sigma ^2}}{\sqrt{2\pi\sigma ^2}}.\]
                Maximizing $\mathcal{L}$ is equivalent to minimizing $-\ln\left(\mathcal{L}\right)$. 
                \[-\mathcal{L}\left(x_1, \ldots, x_n | \mu, \sigma ^2\right) =\frac{n}{2}\ln\left(2\pi\sigma ^2\right) + \sum_{ i =1  } ^{ n }\frac{\left(x_i - \mu\right)^2}{2\sigma ^2}.\]
                For any fixed $\sigma ^2$, the minima of this expression is reached at the empirical mean: $\mu_* = \frac{1}{n}\sum_{ i=1 } ^{ n }x_i$ (which is found by finding the critical ponts as a function of $\mu$). Then, the minima is reached at the empirical variance: $\sigma ^2 =\frac{1}{n}\sum_{ i=1 } ^{ n }\left(x_i - \mu_* \right)^2$ (which is again found by finding the critical points as a function of $\sigmag$). The maximum likelihood estimator of $\left(\mu, \sigma ^2\right)$ is thus given by 
                \[MLE\left(X_1, \ldots, X_n\right)=\left(\bar{X}_n, \hat{\sigma}_n^2\right).\]
            \end{tcolorbox}
        \end{remark}
\section{Quantile tables}
    In the topic of the next section, we will often be confronted to problem of this form: 
    \[P\left(X \leq t\right)=0.95,\]
    To avoid doing computations every time, people have computed thes values and made quantile tables.\
    \begin{definition}
        Let X be a random variables, $q > 0$ be an integer. For integer $k\geq1, t \in \mathbb{R}$ is a \important{kth q-quantile} of X if 
        \[P\left(X <t\right) \leq \frac{k}{q} \text{ AND } P\left(X\leqy\right)\geq\frac{k}{q}.\]
        Of X is a continuous random variabl with strictly positive density, there is only one kth q-quantile of X. Sometimes, one speaks about v-quantiles with v $\in \left(0,1\right)$. In this case, a kth v-quantile of X is a number $t \in \mathbb{R}$ such that 
        \[P\left(X <t\right) \leq kv \text{ AND } P\left(X \leq t\right) \geq kv.\]
    \end{definition}
    \begin{remark}{Example}
        \begin{tcolorbox}[gris]
            The 10-quantiles of the $\mathcal{N}\left(0,1\right)$ distribution are given in the next table. See the graph after for an illustration of the concept. In the table, $t_k$ is the number such that $P\left(X \leq t\right)=\frac{k}{10}$ where $ X \sim \mathcal{N}\left(0,1\right)$.
        \end{tcolorbox}
        \begin{center}
        \includegraphics[width=10cm]{images/3.png}
        \includegraphics[width=8cm]{images/4.png}
        \end{center}
    \end{remark}
\section{Confidence intervals}
    Confidence intervals will allow to state things such as ``given the measurements $X_1, \ldots, X_n$, the parameter $\theta$ belongs to the interval $I\left(X_1, \ldots, X_n\right)$ with probability $p\left(X_1, \ldots, X_n\right)$'', where I, p, are to be sepcified. One is typically interested in finding intervals with very short length (precise estimation) with p close to 1 (small probability of error).
    \begin{definition}
        Let $X_1, \ldots, X_n$ be a sample of law $\mathbb{P}_{\theta}$. Let $\alpha \in \left(0,1\right)$. A ranndom interval $ I = I\left(X_1, \ldots, X_n\right)$ note depending on $\theta$ is called a \important{level $1- \alpha$ confidence interval fo $f\left(\theta\right)$} if for every $\theta \in \Theta$, 
        \[P\left(f\left(\theta\right) \in I \left(X_1, \ldots, X_n\right)\right)=1-\alpha.\]
        $1-\alpha$ is called the \important{confidence level} of the estimation.
    \end{definition}
    \begin{definition}
        A confidence interval $I = I \left(X_1, \ldots, X_n\right)$ is an \important{excess confidence interval for $f\left(\theta\right)$ at level $1 -\alpha$} if 
        \[P\left(f\left(\theta\right) \in I \left(X_1, \ldots, X_n\right)\right) \geq 1- \alpha.\]
        
    \end{definition}
\section{Hypothese testing}
    \subsection{General principle}
        Let's start with an example.
        \begin{remark}{Example}
            \begin{tcolorbox}[gris]
                Let 
                \begin{itemize}[left=10pt, label=\textbullet]
                    \item $p_1$ be the probability that a pipe frome company 1 breaks,
                    \item $p_2$ be the probability that a pipe frome company 2 breaks.
                \end{itemize}
                The parameter is $\theta=\left(p_1, p_2\right)$ with parameter space $\Theta=\left[0,1\right]^2$. Our goal is not to estimate $\theta$ precisely but to determine which region of the parameter space it belongs to.

                \noindent\textbf{Hypotheses:}
                \begin{itemize}[left=10pt, label=\textbullet]
                    \item Null hypothesis $H_0: \theta \in \Theta_0=\left\{\left(p_1,p_2\right):p_1>p_2\right\}$, 

                        (company 1 produces less safes pipes). 
                    \item Alternative hypothesis $H_1: \theta \in \Theta = \left\{\left(p_1, p_2\right): p_1 \leq p_2\right\}$.

                        (company 1 is at least as safe as company 2).
                \end{itemize}
                Based on the data, we decide whether to reject $H_0$. Two types of error may occur: 
                \begin{itemize}[left=10pt, label=\textbullet]
                    \item \textbf{Type I error:} Rejecting $H_0$ when it is true \textrightarrow serious consequence.
                    \item \textbf{Type II error:} not rejecting $H_0$ when it is false \textrightarrow minor consequence.
                \end{itemize}
            \end{tcolorbox}
        \end{remark}
        \textbf{General framework:} we are trying to decide whether a parameter $\theta$ belongs to a region $\Theta_0 \subset \Theta$ or not, based on a sample $X_1, \ldots, X_n$ of law $\mathbb{P_\theta}$.
        \begin{definition}
            The hypotheses ``$\theta \in \Theta_0$'', usually denotes $H_0$, is called the \important{null hypotheses}, Its complement, the hypotheses ``$\theta\in\Theta \backslash \Theta_0$'', usually denoted $H_1$, is called the \important{alternative hypotheses}.
        \end{definition}
        \begin{definition}
            A \important{rejection region D} is an event for the random variables $X_1, \ldots, X_n$. I.e: if the $X_i$ take values in $\mathbb{R}^d, \mathspace D \subset \left(\mathbb{R}^d\right)^n$. In practice, one usually takes 
            \[D = \left\{\left(x_1, \ldots, x_n\right):T\left(x_1, \ldots, x_n\right)\in \left[a,b\right]\right\}\]
            for some statistic T and real numbers $a \leq b$.
        \end{definition}
        \begin{definition}
            Given D a rejection region, and $H_0, H_1$ two hypotheses that are tested one against the other, a \important{test procedure} corresponds to 
            \begin{enumerate}[left=10pt]
                \item reject $H_0$ if $\left(X_1, \ldots, X_n\right) \in D$; 
                \item do not reject $H_0$ if $\left(X_1, \ldots, X_n\right) \in D$.
            \end{enumerate}
            Failure of prediciton are divided into two classes:
            \begin{itemize}[left=10pt, label=\textbullet]
                \item \important{Type-I error}: we reject $H_0$ whereas it was correct. 
                \item \important{Type-II error}: we do not reject $H_0$ whereas it was false.
            \end{itemize}
        \end{definition}
        \begin{definition}
            Let $\alpha \in \left[0,1\right]$. We say that the test procedure has a \important{risk level $\alpha$}, or a \important{confidence level $1-\alpha$} if 
            \[sup_{\theta\in\Theta_0}P\left(\left(X_1, \ldots, X_n\right)\inD\right)=\alpha.\]
        \end{definition}
        \begin{definition}
            The \important{power} of a test is given by 
           \[inf_{\theta \in \Theta_1}P\left(\left(X_1, \ldots, X_n\right) \in D\right) = 1-\beta.\]
           In particular, 
           \[\beta = sup_{\theta \in \Theta_1}P\left(\left(X_1, \ldots, X_n\right) \notin D\right).\]
        \end{definition}
        In words: the risk level $\alpha$ is the ``worst case scenario'' of the probability to fall in the rejection region whilst having a value of the parameter satisfying $H_0$ (type-I error), and the $\beta$ in the power of a test is the ``worst case scenario'' of the probability to fall outside of the rejection region whilst having a value of the parameter not satisfying $H_0$ (type-II error).
    
    \subsection{Chi-square distribution}
        \begin{definition}
            Let $k \in \mathbb{N}^*$ be a positive integer. A random variable X follows the $\mathcal{X}^2$ \important{distribution with k degrees of freedom}, denoted $X \sim \mathcal{X}_k^2$, if it is a continuous random variable with density given by 
            \[f_X\left(x\right) = \mathbbm{1}_{\left[0, +\infty\right)}\left(x\right)\frac{x^{\frac{k}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{k}{2}}\Gamma\left(\frac{k}{2}\right)}.\]
            Recall the Gamma functiong $\Gamma\left(z\right)=\int^\infty_0 x^{z-1}e^{-x}dx$. For $n \in \mathbb{N}^*$, we have $\Gamma\left(n\right)=\left(n-1\right)$!
        \end{definition}
        This law have links with the Gaussian random variables,
        \begin{itemize}[left=10pt, label=\textbullet]
            \item if $k\geq1$ is an integer, and $X_1, \ldots, X_k$ are independent $\mathcal{N}\left(0,1\right)$ random variables, then 
            \[\sum_{ i=k } ^{ k }X_i^2\sim X_k^2.\]
            
            \item if $k\geq1$ is an integer, $\mu \in \mathbb{R}, \sigma ^2 >0$, and $X_1, \ldots, X_k$ are independent $\mathcal{N}\left(\mu, \sigma ^2\right)$ random variables, then the following rescaling of the emprical variance follows a $X_{k-1}^2$ law:
                \[\frac{1}{\sigma ^2}\sum_{ i=1 } ^{ k }\left(X_i=\frac{1}{k}\sum_{ i=1 } ^{ k }X_i\right)^2 \sim X_{k-1}^2.\]      
        \end{itemize}
        \begin{definition}
            Let $k\geq2$ be a positive integer. Let $n\geq1$. Let $p_1, \ldots, p_k \in \left[0,1\right]$ be such that $\sum_{ i=1 } ^{ k }p_i=1$. A random vector $X=\left(X_1, \ldots, X_k\right)$ follows the \important{multinomial distribution with parameters} $\left(k;n;p_1, \ldots, p_k\right)$ if it is a discrete random vector with mass function given by 
            \[\begin{functionbypart}{P\left(X=\left(x_1, \ldots,x_k\right)\right)}
                \frac{n!}{x_1!\ldots x_k!}p_1^{x_1} \ldots p_k^{x_k} \text{   if } x_1, \ldots, x_k \in \mathbb{N}, \sum_{ i=1 } ^{ k }x_i=n,  \\
                0 \mathspace \mathspace \mathspace \text{    else.}
            \end{functionbypart}
            \]
            When $k=2$, its reduces to the binomial distribution.
        \end{definition}
        The convergence result we will use is the following.
        \begin{theoreme}
            Let $k \geq 2, \mathspace p_1, \ldots, p_k \in \left(0,1\right)$ such that $p_1+\ldots+p_k=1$. For $n\geq1$, let $\left(N_{n,1}, \ldots, N_{n,k}\right)$ be a multinomial random vector with parameters $\left(n,k; \mathspace p_1, \ldots, p_k\right)$. Then the random variable 
            \[\sum_{ i=1 } ^{ k }\frac{\left(N_{n,i}-np_i\right)^2}{p_i}\]
            converges in law to a $\mathcal{X}^2$ with $k-1$ degrees of freedom: 
            \[\sum_{ i-1 } ^{ k }\frac{\left(N_{n,i}-np_i\right)^2}{p_i} \xrightarrow{\text{Law}} \mathcal{X}_{k-1}^2, \text{ as } n\to \infty.\]
        \end{theoreme}
        This is used as follows. Let $X_1, \ldots, X_n:\Omega\to\mathbb{R}$ be some independent sequence of i.i.d. Let $A_1, \ldots, A_k \subset \mathbb{R}$ be a partition of $\mathbb{R}$: 
        \[A_i \cap A_j = \emptyset \text{ if } i \neq j, \mathspace \mathspace \cup^k_{i=1}A_i=\mathbb{R}.\]
        Then, the random vector 
        \[\left(\sum_{ i=1 } ^{ n }\mathbbm{1}_{A_1}\left(X_i\right),\sum_{ i=1 } ^{ n }\mathbbm{1}_{A_2}\left(X_i\right), \ldots,\sum_{ i=1 } ^{ n }\mathbbm{1}_{A_k}\left(X_i\right)\right),\]
        which simply counts the number of measurements which fell into each class, follows a multinomila distribution with parameters $\left(n, k; P\left(X \inA_1\right), \ldots, P\left(X_1 \in A_k\right)\right)$. 
        
        
        
        
        
        
        
        
        
        
    
    
    
    
    
        
        
        
        
        
        
             
        
            
         
            
        
        

