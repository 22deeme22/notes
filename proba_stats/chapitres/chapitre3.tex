\chapter{Statistics: starter guide}
\section{Basic objects}
    \subsection{Statistics}
        \begin{definition}
            Let P be a probability measure on $\mathbb{R}^d$. Let $n\geq1$. A \important{size n sample} of law P is an i.i.d. sequence $X_1, \ldots, X_n$ of random variables/vectors having law P. A \important{realisation} of a size n sample is a sequence of values of the random variables: that is a sequence $\left(x_1, \ldots, x_n\right) \in \left(\mathbb{R}^d\right)^n$.
        \end{definition}
        \begin{definition}
            A \important{statistic} on a size n sample $X_1, \ldots, X_n$ with values in $\mathbb{R}^d$ is a random variable $Y:\left(\mathbb{R}^d\right)^n \to \mathbb{R}$.
        \end{definition}
    \subsection{Estimators}
        \begin{definition}
            We are interested in \important{parametric estimation}. We will denote $\mathbb{P}_{\theta}$ the law that we are trying to approximate, $\mathbb{P}_{\theta}$ is thus a probability measure on $\mathbb{R}^d$. We will suppose that it is entirely determined by a parameter $\theta \in \Theta$, where $\Theta \subset \mathbb{R}^m$ for some $ m\geq1$ is the \important{parameter space}.
        \end{definition}
        \begin{remark}{Examples}
            \begin{tcolorbox}[gris]
                We could for example look at 
                \begin{enumerate}[left=10pt]
                    \item a squence of height measurements in the Swiss population, 
                    \item a sequence of life duration for computers in a company ,
                    \item the number of blue cars exiting the highway at Ecublens during a day.
                \end{enumerate}
                We can now chose an apprriate family of probability measures as candidates for approximating each of these situations.
                \begin{enumerate}[left=10pt]
                    \item One can use a Gaussian law $\mathbb{P}_{\theta} = \mathcal{N}\left(\mu, \sigma ^2\right)$. The parameter defining the law is $\theta = \left(\mu, \sigma\right)$. It leads to the parameter space $\Theta = \mathbb{R} x \left[0, +\infty\right).$ 
                    \item One can use an exponential law $\mathbb{P}_{\theta} = \text{Exp}\left(\lambda\right)$. The parameter is then $\theta = \lambda$, and the parameter space is $\Theta = \left(0, +\infty\right)$. 
                    \item One can use a Poisson law $\mathbb{P}_{\theta}=\text{Poi}\left(\lambda\right)$. The parameter is then $\theta = \lambda$, and the parameter space is $\Theta = \left[0, +\infty\right)$.
                \end{enumerate}
            \end{tcolorbox}
        \end{remark}
        \begin{definition}
            Let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$. For $f : \Theta \to \mathbb{R}$, an \important{estimator} of $f\left(\theta\right)$ is a statistic $\hat{f}:\left(\mathbb{R}^d\right)^n \to f\left(\Theta\right)$, which does not depend on the parameter $\theta$. In words, an estimator is a way to probe the parameter space without prior knowledge of the parameter value to approximate the parameter defining our probability law $\mathbb{P}_{\theta}$.
        \end{definition}
        The first notion makes precise the idea that as we increase a sample size, we can find better and better approximations of our parameter.
        \begin{definition}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. We say that the sequence of estimators $\left(\hat{f}_n\right)_{n >=1}$ is \important{convergent} if $\hat{f}_n$ converges in probability towards $f\left(\theta\right): $ for any $\epsilon >0$ and for any $\theta \in \Theta$, 
            \[\lim_{n \to \infty} P\left(\left|\hat{f}_n\left(X_1, \ldots, X_n\right)-f\left(\theta\right)\right| \geq \epsilon\right)=0\]
        \end{definition}
        The next notions ar formalizations of the estimator gives the correct value on average.
        \begin{definition}
            Let $X_1, \ldots, X_n$ be an n-sample of law $\mathbb{P}_{\theta}$. Let $\hat{f}$ be an estimator of $f\left(\theta\right)$. The \important{bias of }$\hat{f}$ is the difference between the expected value of $\hat{f}$ and $f\left(\theta\right)$: 
            \[\text{Bias}_{\theta}\left(\hat{f}\right)=E\left(\hat{f}\left(X_1, \ldots, X_n\right)\right)-f\left(\theta\right).\]
            $\hat{f}$ is called \important{unbiased} if for any $\theta \in \Theta$, $\text{Bias}_{\theta}\left(\hat{f}\right)=0$. Otherwise, it is called \important{biased}.
        \end{definition}
        \begin{definition}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. We say that the sequence of estimators $\left(\hat{f}_n\right)_{n \geq1}$ is \important{asymptotically unbiased} if for any $\theta \in \Theta$, 
            \[\lim_{n \to \infty} \text{Bias}_{\theta}\left(\hat{f}_n\right)=0.\]
        \end{definition}
        \begin{definition}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. The \important{variance} of $\hat{f}_n$, denoted $\text{Var}_{\theta}\left(\hat{f}_n\right)$, is simply the variance of the random variable $\hat{f}_n\left(X_1, \ldots, X_n\right)$: 
            \[\text{Var}_{\theta}\left(\hat{f}_n\right)=\text{Var}\left(\hat{f}_n\left(X_1, \ldots, X_n\right)\right).\]
        \end{definition}
        We can give a first example of criterion guaranteeing that a sequence of estimators converges.
        \begin{theoreme}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. Suppose that
            \begin{enumerate}[left=10pt]
                \item $\hat{f}_n$ is asymptotically unbiased, 
                \item Var$\left(\hat{f}_n\right) \xrightarrow{n \to \infty} 0.$
            \end{enumerate}
            Then, $\hat{f}_n$ is a convergent sequence of estimators.
        \end{theoreme}
    \subsection{Some examples of estimators}
        \subsubsection{Empirical mean}
            The empirical mean is the estimator we already encountered several times. It is an estimator for the expected value and it is given by 
            \[\bar{X}_n = \frac{1}{n}\sum_{ i=1 } ^{ n }X_i.\]
            It is so widelyused that is got its own standard notation. It is diret to see that this estimator is unbiased: 
            \[E\left(\frac{1}{n}\sum_{ i=1 } ^{ n }X_i\right)=\frac{1}{n}\sum_{ i=1 } ^{ n }E\left(X_i\right)=E\left(X_1\right),\]
            as the $X_i$'s are identically distributed. Moreover, the weak Law of Large Numbers implies that it is a convergent estimator as soon as $E\left(X_1\right)$ is well defined.        
        \subsubsection{Empirical median}
            A median for a random variable X is a number M such that 
            \[P\left(X\leqM\right)=P\left(X\geq M\right)=\frac{1}{2}.\]
            The empirical median is obtained as follows.
            \begin{enumerate}[left=10pt]
                \item Order the sample $X_1, \ldots, X_n$ in a non-decreasing fashion: let $\left(\widetilde{X}_1, \ldots, \widetilde{X}_n\right)$ be a permutation of $X_1, \ldots, X_n$ such that $\widetilde{X}_{i+1}\geq \widetilde{X}_i$ for all i's.
                \item Define the empirical median of $X_1, \ldots, X_n$ to be 
                \[
                \begin{functionbypart}{}
                    \widetilde{X}_{\frac{n+1}{2}} \mathspace \mathspace \mathspace \mathspace \text{ if } n \text{ is odd,}  \\
                    \frac{1}{2}\left(\widetilde{X}_{\frac{n}{2}}+\widetilde{X}_{1+\frac{n}{2}}\right) \mathspace \text{ if } n \text{ is even.}
                \end{functionbypart}
            \]
            \end{enumerate}
            One can show that when the law $\mathbb{P}_{\theta}$ is symmetric around its mean (if it is a continuous law with density $f_{\theta}$, this means $f_{\theta}\left(\mu+x\right)=f_{\theta}\left(\mu-x\right)$ with $\mu=\int_{-\infty}^{+\infty}xf_{\theta}\left(x\right)dx$, the median is an unbiased estimator of the mean.
        \subsubsection{Empirical variance}
            We already have an estimator for the expectation: $\bar{X}_n$. A natural thing to do is to define the empirical variance as 
            \[\sigma_n ^2=\frac{1}{n}\sum_{ i=1 } ^{ n }\left(X_i-\bar{X}_n\right)^2=\frac{1}{n}\sum_{ i=1 } ^{ n }X_i ^2-\bar{X}^2_n=\bar{X^2}_n-\bar{X}^2_n.\]
            I.e: take the difference between the empirical mean of $X_1^2, \ldots, X^2_n$ and the square of the empirical mean of $X_1, \ldots, X_n$.
        \subsubsection{Empirical covariance}
            Consider an n-sample $\left(X_1, Y_1\right), \ldots, \left(X_n, Y_n\right)$ of random vectors of law $\mathbb{P}_{\theta}$. The goal is to estimate the covariance $\text{Cov}\left(X,Y\right)$ where $\left(X,Y\right) \sim \mathbb{P}_{\theta}$. Define the estimator 
            \[\hat{\tau}_n\left(\left(X_1, Y_1\right), \ldots, \left(X_n, Y_n\right)\right)=\frac{1}{n}\sum_{ i=1 } ^{ n }X_iY_i-\frac{1}{n}\left(\sum_{ i=1 } ^{ n }X_i\right)\frac{1}{n}\left(\sum_{ i=1 } ^{ n }Y_i\right)=\bar{XY}_n - \bar{X}_n\bar{Y}_n.\]
\section{Constructing estimators}
    \subsection{Moments method}
        Suppose one wants to estimate a parameter $\theta$ from a sample $X_1, \ldots, X_n$ of law $\mathbb{P}_{\theta}$. The general idea is 
        \begin{enumerate}[left=10pt]
            \item find functions h,g such that we have the relation 
            \[\theta = h\left(E\left(g\left(X\right)\right)\right)\]
            with $X \sim \mathbb{P}_{\theta}$; 
            \item use the empirical mean $\frac{1}{n}\sum_{ i=1 } ^{ n }g\left(X_i\right)$ to estimate $E\left(g\left(X\right)\right)$; 
            \item use the estimator of $\theta$ given by 
            \[\hat{\theta}_n = h\left(\frac{1}{n}\sum_{ i=1 } ^{ n }g\left(X_i\right)\right).\]
        \end{enumerate}
        \begin{remark}{Example}
            \begin{tcolorbox}[gris]
                We can look at the case of $\mathbb{P}_{\theta}=\text{Geo}\left(p\right)\left(\text{ so that } \theta =p, \Theta = \left[0,1\right]\right)$.

                We know that if $X \sim \text{Geo}\left(p\right)$, 
                \[E\left(X\right)=\frac{1}{p}, \mathspace E\left(X^2\right)=\frac{2-p}{p^2}.\]
                Each of these give rise to estimator via the moment method.
                \begin{itemize}[left=10pt, label=\textbullet]
                    \item First consider the first moment. We can use $h\left(x\right)=x^{-1}$ and $g\left(x\right)=x$. This gives the estimator of p 
                    \[\hat{p}_n\left(X_1, \ldots, X_n\right)=\frac{n}{\sum_{ i=1 } ^{ n }X_i}.\]
                    
                    \item Then consider the second moment. We can use $h\left(x\right)=\frac{\sqrt{1+8x}-1}{2x}$ and $g\left(x\right)=x^2$.

                        This gives the estimator of p 
                        \[\hat{p}_n\left(X_1, \ldots, X_n\right)=\frac{n}{2 \sum_{ i=1 } ^{ n }X_i^2}\left(\left(1+\frac{8}{n}\sum_{ i=1 } ^{ n }X_i^2\right)^{\frac{1}{2}}-1\right).\]
                        
                \end{itemize}
            \end{tcolorbox}
        \end{remark}
    \subsection{Maximum likelihood estimator}
        This estimator is deeply linked to the Bayes formula: imagine that you have a realisation $x_1, \ldots, x_n$ of a sample $X_1, \ldots, X_n$ of law $\mathbb{P}_{\theta}$. The idea is to look for the value of $\theta$ which maximises the probability to observe the realisation $x_1, \ldots, x_n$.
        \begin{definition}
            Let $n >=1$, and a sample $X_1, \ldots, X_n$ of law $\mathbb{P}_{\theta}$. For a realisation $x_1, \ldots, x_n$ of $X_1, \ldots, X_n$, the \important{likelihood} of $x_1, \ldots, x_n$ given $\theta, \mathspace\mathcal{L}\left(x_1, \ldots, x_n | \theta\right)$, is defined by 
            \begin{itemize}[left=10pt, label=\textbullet]
                \item if $\mathbb{P}_{\theta}$ is a discrete probability measure (i.e.: the $X_i$'s are discrete random variables), 
                    \[\mathcal{L}\left(x_1, \ldots, x_n|\theta\right)=P\left(X_1=x_1, \ldots, X_n = x_n\right)= \prod_{i=1}^{ n } P\left(X_i =x_i\right)=\prod_{i=1}^{ n } \mathbb{P}_{\theta}\left(\{x_i\}\right)\]
                as the $X_i$'s are independent and of law $\mathbb{P}_{\theta}$; 
                \item if $\mathbb{P}_{\theta}$ is a continuous probability measure (i.e.: the $X_i$'s are continuous random variables), 
                \[\mathcal{L}\left(x_1, \ldots, x_n|\theta\right)=\prod_{i=1}^{ n } f_{X_i}\left(x_i\right)=\prod_{i=1}^{ n } f_{\theta}\left(x_i\right)\]
                where $f_{\theta}$ is the density function associated to $\mathbb{P}_{\theta}$.
            \end{itemize}
        \end{definition}
        \begin{definition}
            The \important{maximum likelihood estimator} of $\theta$ is given by 
        \[MLE\left(X_1, \ldots, X_n\right) =argmax_{\theta}\mathcal{L}\left(X_1, \ldots, X_n|\theta\right),\]
            the point $\theta$ at which $\mathcal{L}\left(X_1, \ldots, X_n|\theta\right)$ is maximized.
        \end{definition}
        \begin{remark}{Example}
            \begin{tcolorbox}[gris]
                Consider the case $X_1, \ldots, X_n$ is a n-sample of law $\mathcal{N}\left(\mu, \sigma ^2\right)$ (and take $\theta=\left(\mu, \sigma ^2\right)$). The likelihood function of a realisation $x_1, \ldots, x_n \in \mathbb{N}^*$ is given by 
                \[\mathcal{L}\left(x_1, \ldots, x_n| \mu, \sigma ^2\right) = \prod_{i=1}^{ n } \frac{e^{-\left(x_i-\mu\right)^2 \slash 2\sigma ^2}}{\sqrt{2\pi\sigma ^2}}.\]
                Maximizing $\mathcal{L}$ is equivalent to minimizing $-\ln\left(\mathcal{L}\right)$. 
                \[-\mathcal{L}\left(x_1, \ldots, x_n | \mu, \sigma ^2\right) =\frac{n}{2}\ln\left(2\pi\sigma ^2\right) + \sum_{ i =1  } ^{ n }\frac{\left(x_i - \mu\right)^2}{2\sigma ^2}.\]
                For any fixed $\sigma ^2$, the minima of this expression is reached at the empirical mean: $\mu_* = \frac{1}{n}\sum_{ i=1 } ^{ n }x_i$ (which is found by finding the critical ponts as a function of $\mu$). Then, the minima is reached at the empirical variance: $\sigma ^2 =\frac{1}{n}\sum_{ i=1 } ^{ n }\left(x_i - \mu_* \right)^2$ (which is again found by finding the critical points as a function of $\sigmag$). The maximum likelihood estimator of $\left(\mu, \sigma ^2\right)$ is thus given by 
                \[MLE\left(X_1, \ldots, X_n\right)=\left(\bar{X}_n, \hat{\sigma}_n^2\right).\]
            \end{tcolorbox}
        \end{remark}
\section{Quantile tables}
    In the topic of the next section, we will often be confronted to problem of this form: 
    \[P\left(X \leq t\right)=0.95,\]
    To avoid doing computations every time, people have computed thes values and made quantile tables.\
    \begin{definition}
        Let X be a random variables, $q > 0$ be an integer. For integer $k\geq1, t \in \mathbb{R}$ is a \important{kth q-quantile} of X if 
        \[P\left(X <t\right) \leq \frac{k}{q} \text{ AND } P\left(X\leqy\right)\geq\frac{k}{q}.\]
        Of X is a continuous random variabl with strictly positive density, there is only one kth q-quantile of X. Sometimes, one speaks about v-quantiles with v $\in \left(0,1\right)$. In this case, a kth v-quantile of X is a number $t \in \mathbb{R}$ such that 
        \[P\left(X <t\right) \leq kv \text{ AND } P\left(X \leq t\right) \geq kv.\]
    \end{definition}
    \begin{remark}{Example}
        \begin{tcolorbox}[gris]
            The 10-quantiles of the $\mathcal{N}\left(0,1\right)$ distribution are given in the next table. See the graph after for an illustration of the concept. In the table, $t_k$ is the number such that $P\left(X \leq t\right)=\frac{k}{10}$ where $ X \sim \mathcal{N}\left(0,1\right)$.
        \end{tcolorbox}
        \begin{center}
        \includegraphics[width=10cm]{images/3.png}
        \includegraphics[width=8cm]{images/4.png}
        \end{center}
    \end{remark}
\section{Confidence intervals}
    Confidence intervals will allow to state things such as ``given the measurements $X_1, \ldots, X_n$, the parameter $\theta$ belongs to the interval $I\left(X_1, \ldots, X_n\right)$ with probability $p\left(X_1, \ldots, X_n\right)$'', where I, p, are to be sepcified. One is typically interested in finding intervals with very short length (precise estimation) with p close to 1 (small probability of error).
    \begin{definition}
        Let $X_1, \ldots, X_n$ be a sample of law $\mathbb{P}_{\theta}$. Let $\alpha \in \left(0,1\right)$. A ranndom interval $ I = I\left(X_1, \ldots, X_n\right)$ note depending on $\theta$ is called a \important{level $1- \alpha$ confidence interval fo $f\left(\theta\right)$} if for every $\theta \in \Theta$, 
        \[P\left(f\left(\theta\right) \in I \left(X_1, \ldots, X_n\right)\right)=1-\alpha.\]
        $1-\alpha$ is called the \important{confidence level} of the estimation.
    \end{definition}
    \begin{definition}
        A confidence interval $I = I \left(X_1, \ldots, X_n\right)$ is an \important{excess confidence interval for $f\left(\theta\right)$ at level $1 -\alpha$} if 
        \[P\left(f\left(\theta\right) \in I \left(X_1, \ldots, X_n\right)\right) \geq 1- \alpha.\]
        
    \end{definition}
\section{Hypothese testing}
    \subsection{General principle}
        Let's start with an example.
        \begin{remark}{Example}
            \begin{tcolorbox}[gris]
                Let 
                \begin{itemize}[left=10pt, label=\textbullet]
                    \item $p_1$ be the probability that a pipe frome company 1 breaks,
                    \item $p_2$ be the probability that a pipe frome company 2 breaks.
                \end{itemize}
                The parameter is $\theta=\left(p_1, p_2\right)$ with parameter space $\Theta=\left[0,1\right]^2$. Our goal is not to estimate $\theta$ precisely but to determine which region of the parameter space it belongs to.

                \noindent\textbf{Hypotheses:}
                \begin{itemize}[left=10pt, label=\textbullet]
                    \item Null hypothesis $H_0: \theta \in \Theta_0=\left\{\left(p_1,p_2\right):p_1>p_2\right\}$, 

                        (company 1 produces less safes pipes). 
                    \item Alternative hypothesis $H_1: \theta \in \Theta = \left\{\left(p_1, p_2\right): p_1 \leq p_2\right\}$.

                        (company 1 is at least as safe as company 2).
                \end{itemize}
                Based on the data, we decide whether to reject $H_0$. Two types of error may occur: 
                \begin{itemize}[left=10pt, label=\textbullet]
                    \item \textbf{Type I error:} Rejecting $H_0$ when it is true \textrightarrow serious consequence.
                    \item \textbf{Type II error:} not rejecting $H_0$ when it is false \textrightarrow minor consequence.
                \end{itemize}
            \end{tcolorbox}
        \end{remark}
        \textbf{General framework:} we are trying to decide whether a parameter $\theta$ belongs to a region $\Theta_0 \subset \Theta$ or not, based on a sample $X_1, \ldots, X_n$ of law $\mathbb{P_\theta}$.
        \begin{definition}
            The hypotheses ``$\theta \in \Theta_0$'', usually denotes $H_0$, is called the \important{null hypotheses}, Its complement, the hypotheses ``$\theta\in\Theta \backslash \Theta_0$'', usually denoted $H_1$, is called the \important{alternative hypotheses}.
        \end{definition}
        \begin{definition}
            A \important{rejection region D} is an event for the random variables $X_1, \ldots, X_n$. I.e: if the $X_i$ take values in $\mathbb{R}^d, \mathspace D \subset \left(\mathbb{R}^d\right)^n$. In practice, one usually takes 
            \[D = \left\{\left(x_1, \ldots, x_n\right):T\left(x_1, \ldots, x_n\right)\in \left[a,b\right]\right\}\]
            for some statistic T and real numbers $a \leq b$.
        \end{definition}
        \begin{definition}
            Given D a rejection region, and $H_0, H_1$ two hypotheses that are tested one against the other, a \important{test procedure} corresponds to 
            \begin{enumerate}[left=10pt]
                \item reject $H_0$ if $\left(X_1, \ldots, X_n\right) \in D$; 
                \item do not reject $H_0$ if $\left(X_1, \ldots, X_n\right) \in D$.
            \end{enumerate}
            Failure of prediciton are divided into two classes:
            \begin{itemize}[left=10pt, label=\textbullet]
                \item \important{Type-I error}: we reject $H_0$ whereas it was correct. 
                \item \important{Type-II error}: we do not reject $H_0$ whereas it was false.
            \end{itemize}
        \end{definition}
        \begin{definition}
            Let $\alpha \in \left[0,1\right]$. We say that the test procedure has a \important{risk level $\alpha$}, or a \important{confidence level $1-\alpha$} if 
            \[sup_{\theta\in\Theta_0}P\left(\left(X_1, \ldots, X_n\right)\inD\right)=\alpha.\]
        \end{definition}
        \begin{definition}
            The \important{power} of a test is given by 
           \[inf_{\theta \in \Theta_1}P\left(\left(X_1, \ldots, X_n\right) \in D\right) = 1-\beta.\]
           In particular, 
           \[\beta = sup_{\theta \in \Theta_1}P\left(\left(X_1, \ldots, X_n\right) \notin D\right).\]
        \end{definition}
        In words: the risk level $\alpha$ is the ``worst case scenario'' of the probability to fall in the rejection region whilst having a value of the parameter satisfying $H_0$ (type-I error), and the $\beta$ in the power of a test is the ``worst case scenario'' of the probability to fall outside of the rejection region whilst having a value of the parameter not satisfying $H_0$ (type-II error).
    
    \subsection{Chi-square distribution}
        \begin{definition}
            Let $k \in \mathbb{N}^*$ be a positive integer. A random variable X follows the $\mathcal{X}^2$ \important{distribution with k degrees of freedom}, denoted $X \sim \mathcal{X}_k^2$, if it is a continuous random variable with density given by 
            \[f_X\left(x\right) = \mathbbm{1}_{\left[0, +\infty\right)}\left(x\right)\frac{x^{\frac{k}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{k}{2}}\Gamma\left(\frac{k}{2}\right)}.\]
            Recall the Gamma functiong $\Gamma\left(z\right)=\int^\infty_0 x^{z-1}e^{-x}dx$. For $n \in \mathbb{N}^*$, we have $\Gamma\left(n\right)=\left(n-1\right)$!
        \end{definition}
        This law have links with the Gaussian random variables,
        \begin{itemize}[left=10pt, label=\textbullet]
            \item if $k\geq1$ is an integer, and $X_1, \ldots, X_k$ are independent $\mathcal{N}\left(0,1\right)$ random variables, then 
            \[\sum_{ i=k } ^{ k }X_i^2\sim X_k^2.\]
            
            \item if $k\geq1$ is an integer, $\mu \in \mathbb{R}, \sigma ^2 >0$, and $X_1, \ldots, X_k$ are independent $\mathcal{N}\left(\mu, \sigma ^2\right)$ random variables, then the following rescaling of the emprical variance follows a $X_{k-1}^2$ law:
                \[\frac{1}{\sigma ^2}\sum_{ i=1 } ^{ k }\left(X_i=\frac{1}{k}\sum_{ i=1 } ^{ k }X_i\right)^2 \sim X_{k-1}^2.\]      
        \end{itemize}
        \begin{definition}
            Let $k\geq2$ be a positive integer. Let $n\geq1$. Let $p_1, \ldots, p_k \in \left[0,1\right]$ be such that $\sum_{ i=1 } ^{ k }p_i=1$. A random vector $X=\left(X_1, \ldots, X_k\right)$ follows the \important{multinomial distribution with parameters} $\left(k;n;p_1, \ldots, p_k\right)$ if it is a discrete random vector with mass function given by 
            \[\begin{functionbypart}{P\left(X=\left(x_1, \ldots,x_k\right)\right)}
                \frac{n!}{x_1!\ldots x_k!}p_1^{x_1} \ldots p_k^{x_k} \text{   if } x_1, \ldots, x_k \in \mathbb{N}, \sum_{ i=1 } ^{ k }x_i=n,  \\
                0 \mathspace \mathspace \mathspace \text{    else.}
            \end{functionbypart}
            \]
            When $k=2$, its reduces to the binomial distribution.
        \end{definition}
        The convergence result we will use is the following.
        \begin{theoreme}
            Let $k \geq 2, \mathspace p_1, \ldots, p_k \in \left(0,1\right)$ such that $p_1+\ldots+p_k=1$. For $n\geq1$, let $\left(N_{n,1}, \ldots, N_{n,k}\right)$ be a multinomial random vector with parameters $\left(n,k; \mathspace p_1, \ldots, p_k\right)$. Then the random variable 
            \[\sum_{ i=1 } ^{ k }\frac{\left(N_{n,i}-np_i\right)^2}{p_i}\]
            converges in law to a $\mathcal{X}^2$ with $k-1$ degrees of freedom: 
            \[\sum_{ i-1 } ^{ k }\frac{\left(N_{n,i}-np_i\right)^2}{p_i} \xrightarrow{\text{Law}} \mathcal{X}_{k-1}^2, \text{ as } n\to \infty.\]
        \end{theoreme}
        This is used as follows. Let $X_1, \ldots, X_n:\Omega\to\mathbb{R}$ be some independent sequence of i.i.d. Let $A_1, \ldots, A_k \subset \mathbb{R}$ be a partition of $\mathbb{R}$: 
        \[A_i \cap A_j = \emptyset \text{ if } i \neq j, \mathspace \mathspace \cup^k_{i=1}A_i=\mathbb{R}.\]
        Then, the random vector 
        \[\left(\sum_{ i=1 } ^{ n }\mathbbm{1}_{A_1}\left(X_i\right),\sum_{ i=1 } ^{ n }\mathbbm{1}_{A_2}\left(X_i\right), \ldots,\sum_{ i=1 } ^{ n }\mathbbm{1}_{A_k}\left(X_i\right)\right),\]
        which simply counts the number of measurements which fell into each class, follows a multinomila distribution with parameters $\left(n, k; P\left(X \inA_1\right), \ldots, P\left(X_1 \in A_k\right)\right)$. 
    \subsection{$\mathcal{X}^2$ tests}
        \subsubsection{Adequacy tests}
            Adequacy tests are a way to check whether the observed realisation of the sample was obtain from a given law or not. Consider an experiment with a certain realisation space $\Omega$. Suppose that you can partition $\Omega$ into disjoint classes $\Omega_1, \ldots, \Omega_k$. If we let $\mathbb{P}$ be the lae of the sample (which we do not know), we then have the probability of falling in each of the classes is a number $p_i = \mathbb{P}\left(\Omega_i\right) \in \left[0,1\right]$, and that $\sum_{ i=1 } ^{ k }p_i=1$. Observing a realisation of a n-sample $X_1, \ldots, X_n$, we can associate the numbers of times weobserved a realisation in the class $\Omega_i$: 
            \[N_{n,i}\left(x_1, \ldots, x_n\right)=\sum_{ j=1 } ^{ n }\mathbbm{1}_{\Omega_i}\left(x_j\right).\]
            If we assume that the law of the sample is $\mathbb{P}$, we have thtat the vector $\left(N_{n,1}, \ldots, N_{n,k}\right)$ folllows a \important{multinomial distribution} of parameters $\left(n,k ; p_1, \ldots, p_k\right)$: recall that it is a discrete random vector with, for $m_1, \ldots, m_k \in \mathbb{N}$, 
            \[P\left(\left(N_{n,1}, \ldots, N_{n,k}\right)=\left(m_1, \ldots, m_k\right)\right)=\mathbbm{1}_{m_1+\ldots+m_k=n}\frac{n!}{m_1!\ldots m_k!}p_1^{m_1}\ldots p_k^{m_k}.\]
            We can now describe the idea of the test. Our goal is to test wether $\mathbb{P}$, the law of the sample, is a certain law $\mathbb{Q}$ or not. We thus introduce 
            \[q_i=\mathbb{Q}\left(\Omega_i\right), \mathspace i=1, \ldots, k.\]
            We then have that if $\mathbb{P}=\mathbb{Q}, \mathspace p_i=q_i$ for every $i=1, \ldots,k$ \ldots but if $p_i \neq q_i$ for some i, then we know for sure that $\mathbb{P}\neq\mathbb{Q}$! We will therefore test the null hypotheses $H_0$: ``$p_i=q_i$ for $i =1, \ldots,k$'' against the alternative hypotheses $H_1$ `` there is $i \in \left\{1, \ldots, k\right\}$ such that $p_i \neq q_i$''. 

            \noindent Under $H_0$, the $q_i$'s should be well approximated by the empirical frequencues: $q_i \approx N_{n,i} \backslash n$. We just transformed a non-parametric question ``is $\mathbb{P}$ equal to $\mathbb{Q}$? into a parametric one about the parameters of a multinomial law!

            We now need to construct our rejection region. Introduce the statistic 
            \[Z_n\left(X_1, \ldots, X_n\right)=\sum_{ i=1 } ^{ k }\frac{\left(N_{n,i}\left(X_1, \ldots, X_n\right)-nq_i\right)^2}{nq_i}=n \sum_{ i=1 } ^{ k }\frac{\left(\frac{N_{n,i}\left(X_1, \ldots, X_n\right)}{n}-q_i\right)^2}{q_i}.\]
            It is some measure of the difference between the observed frequencies $\frac{N_{n,i}\left(X_1, \ldots, X_n\right)}{n}$ and the ones we should see under the null hypotheses. The reason to chose this particular statistic is the lemma thats, in our setup, says that, under the null Hypotheses $H_0$, 
            \[Z_n\left(X_1, \ldots, X_n\right)\xrightarrow{\text{Law}} \mathcal{X}_{k-1}^2.\]
            From this Lemma, we have on the one hand under $H_0, Z_n$ follows asymptotically a $\mathcal{X}^2$ law with $k-1$ degrees of freedom. On the onther hand, under $H_1$, there is $i_{*} \in \left\{1, \ldots, k\right\}$ such that 
            \[\left(\frac{N_{n,i_{*}}}{n}-q_{i_*}\right)^2 \xrightarrow{\text{a.s.}}\left(p_{i_*}-q_{i_*}\right)^2 >0,\]
            as $n\to \infty$, which implies that $Z_n \xrightarrow{\text{a,s,}} +\infty$. If we want a risk level $\alpha$ for our test, we can thus take a rejection region of the form $D=\left\{Z_n >C\right\}$ with C such that, with $Z \sim \mathcal{X}_{k-1}^2$, 
            \[P\left(Z>C\right)=\alpha.\]
            This gives a region with risk level $\alpha$ asymptotically as, under $H_0$, 
            \[P\left(Z_n>C\right)\xrightarrow{n\to \infty}P\left(Z>C\right).\]
        \subsubsection{Independence test}
            We will only consider an example of this test and not describe the general theory. The
            goal of the test is to test the hypotheses ``are property A and property B independent?''.
            Here are the measures of hairs and eyes colour in a group of people.
            \begin{center}
            \includegraphics[width=10cm]{images/5.png}
            \end{center}
            We then want to test wether the colour of the eyes is independent of the colour of the hairs. Let the null hypotheses be ``the eyes and hairs colours are independent'' against the alternative hypotheses.
            
            Denote the eyes colours by 
            \[1 \equiv \text{ blue, } \mathspace 2 \equiv \text{ grey, } \mathspace 3 \equiv \text{ brown.}\]
            and the hair colours by 
            \[1 \equiv \text{ blond, } \mathspace 2 \equiv \text{   brown, }\mathspace 3 \equiv\text{ black, } \mathspace 4 \equiv\text{ ginger.}\]
            Denote then $p_{ij}$ the probability that an individual has eyes colour i and hair colour j and set 
            \[p_{i*} = \sum_{ i=1 } ^{ 4 }P_{ik}, \mathspace p_{*j}=\sum_{ j=1 } ^{ 3 }p_{kj},\]
            the probability that an individual has eye colour i, hair colour j. Under the null hypotheses, we have that hair and eyes colours are independent, which translates into 
            \[p_{ij}=p_{i*}p_{*j}, \mathspace i =1,2,3, \mathspace j=1,2,3,4.\]
            We therefore have that the null hypotheses can be re-phrased ``$p_{ij}=p_{i*}p_{*j}$ for every $i \in \left\{1,2,3\right\}$, and every $j \in \left\{1,2,3,4\right\}$'', which turns out to be a parametric hypotheses.

            Under the null hypotheses, we have that the number of individual with eye colour
            i and hair colour j in a sample of n people is on average 
            \[np_{ij}=np_{i*}p_{*j}.\]
            Also, we know from the LLN that $p_{i*}$ is well approximated by the empirical frequency. We can then test this hypotheses as before by introducing the statistic 
            \[Z_n = \sum_{ i=1 } ^{ 3 }\sum_{ j=1 } ^{ 4 }\frac{\left(N_{ij}-n\frac{N_{i*}N_{*j}}{n^2}\right)^2}{n\frac{N_{i*}N_{*j}}{n^2}}=n \sum_{ i=1 } ^{ 3 }\sum_{ j=1 } ^{ 4 }\frac{\left(\frac{N_{ij}}{n}-\frac{N_{i*}N_{*j}}{n^2}\right)^2}{\frac{N_{i*}N_{*j}}{n^2}}\]
            Where $N_{ij}$ counts the number of individual eye colour i and hair colour j, $N_{i*}$ counts the number of individual with eye colour i, and $N_{*j}$ with haire colour j. Under the null hypotheses, $Z_n$ converges to a $\mathcal{X}_6^2$ law. 

            Using our data in $Z_{124}$ with a rejection region at risk $5\%$, we obtain that we should reject the null hypotheses (so no independence).
    \section{t-test}
        \begin{definition}
            Let $v \in \left(0,+\infty\right)$ be a positive real. A random variable X follows the \important{Student's t-distribution with parameter $\nu$}, denoted $X\sim \text{Student}_t\left(\nu\right)$, if it is a continuous random variable with density given by 
            \[f_X\left(x\right)=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\pi\nu}\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}.\]
            Often, one takes $\nu \in \mathbb{N}^*$. In this case, we call $\text{Student}_t\left(\nu\right)$ the \important{Student's t-distribution with $\nu$ degrees of freedom}.       
        \end{definition}
        \begin{theoreme}
            Let $Z,V$ be two independent random variables with 
            \[Z \sim \mathcal{N}\left(0,1\right), \mathspace V\sim\mathcal{X}_k^2, \mathspace k \in \left\{2,3,4, \ldots\right\}.\]
            Then, the random variable $Z\sqrt{\frac{k}{V}}$ follows a Student's t-distribution with parameter k: 
            \[Z\sqrt{\frac{k}{V}}\sim \text{Student}_t\left(k\right). \]
        \end{theoreme}
        \subsubsection{One-sample t-test}
            It is used to test the null Hypotheses thatt the mean of a sample is given by a fixed value.
            
            \textbf{Setup:} we have a sample $X_1, \ldots, X_n$ of a law $\mathbb{P}$. We want to test the Hypotheses $H_0$``$E\left(X_1\right)=\mu_0$ for a fixed $\mu_0 \in \mathbb{R}$.
        
        
        
            
            
            
            
            
            
  
        
        
        
        
        
        
        
        
        
        
        
    
    
    
    
    
        
        
        
        
        
        
             
        
            
         
            
        
        

