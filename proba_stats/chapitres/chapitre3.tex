\chapter{Statistics: starter guide}
\section{Basic objects}
    \subsection{Statistics}
        \begin{definition}
            Let P be a probability measure on $\mathbb{R}^d$. Let $n\geq1$. A \important{size n sample} of law P is an i.i.d. sequence $X_1, \ldots, X_n$ of random variables/vectors having law P. A \important{realisation} of a size n sample is a sequence of values of the random variables: that is a sequence $\left(x_1, \ldots, x_n\right) \in \left(\mathbb{R}^d\right)^n$.
        \end{definition}
        \begin{definition}
            A \important{statistic} on a size n sample $X_1, \ldots, X_n$ with values in $\mathbb{R}^d$ is a random variable $Y:\left(\mathbb{R}^d\right)^n \to \mathbb{R}$.
        \end{definition}
    \subsection{Estimators}
        \begin{definition}
            We are interested in \important{parametric estimation}. We will denote $\mathbb{P}_{\theta}$ the law that we are trying to approximate, $\mathbb{P}_{\theta}$ is thus a probability measure on $\mathbb{R}^d$. We will suppose that it is entirely determined by a parameter $\theta \in \Theta$, where $\Theta \subset \mathbb{R}^m$ for some $ m\geq1$ is the \important{parameter space}.
        \end{definition}
        \begin{remark}{Examples}
            \begin{tcolorbox}[gris]
                We could for example look at 
                \begin{enumerate}[left=10pt]
                    \item a squence of height measurements in the Swiss population, 
                    \item a sequence of life duration for computers in a company ,
                    \item the number of blue cars exiting the highway at Ecublens during a day.
                \end{enumerate}
                We can now chose an apprriate family of probability measures as candidates for approximating each of these situations.
                \begin{enumerate}[left=10pt]
                    \item One can use a Gaussian law $\mathbb{P}_{\theta} = \mathcal{N}\left(\mu, \sigma ^2\right)$. The parameter defining the law is $\theta = \left(\mu, \sigma\right)$. It leads to the parameter space $\Theta = \mathbb{R} x \left[0, +\infty\right).$ 
                    \item One can use an exponential law $\mathbb{P}_{\theta} = \text{Exp}\left(\lambda\right)$. The parameter is then $\theta = \lambda$, and the parameter space is $\Theta = \left(0, +\infty\right)$. 
                    \item One can use a Poisson law $\mathbb{P}_{\theta}=\text{Poi}\left(\lambda\right)$. The parameter is then $\theta = \lambda$, and the parameter space is $\Theta = \left[0, +\infty\right)$.
                \end{enumerate}
            \end{tcolorbox}
        \end{remark}
        \begin{definition}
            Let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$. For $f : \Theta \to \mathbb{R}$, an \important{estimator} of $f\left(\theta\right)$ is a statistic $\hat{f}:\left(\mathbb{R}^d\right)^n \to f\left(\Theta\right)$, which does not depend on the parameter $\theta$. In words, an estimator is a way to probe the parameter space without prior knowledge of the parameter value to approximate the parameter defining our probability law $\mathbb{P}_{\theta}$.
        \end{definition}
        The first notion makes precise the idea that as we increase a sample size, we can find better and better approximations of our parameter.
        \begin{definition}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. We say that the sequence of estimators $\left(\hat{f}_n\right)_{n >=1}$ is \important{convergent} if $\hat{f}_n$ converges in probability towards $f\left(\theta\right): $ for any $\epsilon >0$ and for any $\theta \in \Theta$, 
            \[\lim_{n \to \infty} P\left(\left|\hat{f}_n\left(X_1, \ldots, X_n\right)-f\left(\theta\right)\right| \geq \epsilon\right)=0\]
        \end{definition}
        The next notions ar formalizations of the estimator gives the correct value on average.
        \begin{definition}
            Let $X_1, \ldots, X_n$ be an n-sample of law $\mathbb{P}_{\theta}$. Let $\hat{f}$ be an estimator of $f\left(\theta\right)$. The \important{bias of }$\hat{f}$ is the difference between the expected value of $\hat{f}$ and $f\left(\theta\right)$: 
            \[\text{Bias}_{\theta}\left(\hat{f}\right)=E\left(\hat{f}\left(X_1, \ldots, X_n\right)\right)-f\left(\theta\right).\]
            $\hat{f}$ is called \important{unbiased} if for any $\theta \in \Theta$, $\text{Bias}_{\theta}\left(\hat{f}\right)=0$. Otherwise, it is called \important{biased}.
        \end{definition}
        \begin{definition}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. We say that the sequence of estimators $\left(\hat{f}_n\right)_{n \geq1}$ is \important{asymptotically unbiased} if for any $\theta \in \Theta$, 
            \[\lim_{n \to \infty} \text{Bias}_{\theta}\left(\hat{f}_n\right)=0.\]
        \end{definition}
        \begin{definition}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. The \important{variance} of $\hat{f}_n$, denoted $\text{Var}_{\theta}\left(\hat{f}_n\right)$, is simply the variance of the random variable $\hat{f}_n\left(X_1, \ldots, X_n\right)$: 
            \[\text{Var}_{\theta}\left(\hat{f}_n\right)=\text{Var}\left(\hat{f}_n\left(X_1, \ldots, X_n\right)\right).\]
        \end{definition}
        We can give a first example of criterion guaranteeing that a sequence of estimators converges.
        \begin{theoreme}
            For $n \geq 1$, let $X_1, \ldots, X_n$ be a n-sample of law $\mathbb{P}_{\theta}$, and let $\hat{f}_n$ be an estimator of $f\left(\theta\right)$. Suppose that
            \begin{enumerate}[left=10pt]
                \item $\hat{f}_n$ is asymptotically unbiased, 
                \item Var$\left(\hat{f}_n\right) \xrightarrow{n \to \infty} 0.$
            \end{enumerate}
            Then, $\hat{f}_n$ is a convergent sequence of estimators.
        \end{theoreme}
    \subsection{Some examples of estimators}
        \subsubsection{Empirical mean}
            The empirical mean is the estimator we already encountered several times. It is an estimator for the expected value and it is given by 
            \[\bar{X}_n = \frac{1}{n}\sum_{ i=1 } ^{ n }X_i.\]
            It is so widelyused that is got its own standard notation. It is diret to see that this estimator is unbiased: 
            \[E\left(\frac{1}{n}\sum_{ i=1 } ^{ n }X_i\right)=\frac{1}{n}\sum_{ i=1 } ^{ n }E\left(X_i\right)=E\left(X_1\right),\]
            as the $X_i$'s are identically distributed. Moreover, the weak Law of Large Numbers implies that it is a convergent estimator as soon as $E\left(X_1\right)$ is well defined.        
        \subsubsection{Empirical median}
            A median for a random variable X is a number M such that 
            \[P\left(X\leqM\right)=P\left(X\geq M\right)=\frac{1}{2}.\]
            The empirical median is obtained as follows.
            \begin{enumerate}[left=10pt]
                \item Order the sample $X_1, \ldots, X_n$ in a non-decreasing fashion: let $\left(\widetilde{X}_1, \ldots, \widetilde{X}_n\right)$ be a permutation of $X_1, \ldots, X_n$ such that $\widetilde{X}_{i+1}\geq \widetilde{X}_i$ for all i's.
                \item Define the empirical median of $X_1, \ldots, X_n$ to be 
                \[
                \begin{functionbypart}{}
                    \widetilde{X}_{\frac{n+1}{2}} \mathspace \mathspace \mathspace \mathspace \text{ if } n \text{ is odd,}  \\
                    \frac{1}{2}\left(\widetilde{X}_{\frac{n}{2}}+\widetilde{X}_{1+\frac{n}{2}}\right) \mathspace \text{ if } n \text{ is even.}
                \end{functionbypart}
            \]
            \end{enumerate}
            One can show that when the law $\mathbb{P}_{\theta}$ is symmetric around its mean (if it is a continuous law with density $f_{\theta}$, this means $f_{\theta}\left(\mu+x\right)=f_{\theta}\left(\mu-x\right)$ with $\mu=\int_{-\infty}^{+\infty}xf_{\theta}\left(x\right)dx$, the median is an unbiased estimator of the mean.
        \subsubsection{Empirical variance}
            We already have an estimator for the expectation: $\bar{X}_n$. A natural thing to do is to define the empirical variance as 
            \[\sigma_n ^2=\frac{1}{n}\sum_{ i=1 } ^{ n }\left(X_i-\bar{X}_n\right)^2=\frac{1}{n}\sum_{ i=1 } ^{ n }X_i ^2-\bar{X}^2_n=\bar{X}^2_n-\bar{X}^2_n.\]
            I.e: take the difference between the empirical mean of $X_1^2, \ldots, X^2_n$ and the square of the empirical mean of $X_1, \ldots, X_n$.
        \subsubsection{Empirical covariance}
            Consider an n-sample $\left(X_1, Y_1\right), \ldots, \left(X_n, Y_n\right)$ of random vectors of law $\mathbb{P}_{\theta}$. The goal is to estimate the covariance $\text{Cov}\left(X,Y\right)$ where $\left(X,Y\right) \sim \mathbb{P}_{\theta}$. Define the estimator 
            \[\hat{\tau}_n\left(\left(X_1, Y_1\right), \ldots, \left(X_n, Y_n\right)\right)=\frac{1}{n}\sum_{ i=1 } ^{ n }X_iY_i-\frac{1}{n}\left(\sum_{ i=1 } ^{ n }X_i\right)\frac{1}{n}\left(\sum_{ i=1 } ^{ n }Y_i\right)=\bar{XY}_n - \bar{X}_n\bar{Y}_n.\]
\section{Constructing estimators}
    \subsection{Moments method}
        Suppose one wants to estimate a parameter $\theta$ from a sample $X_1, \ldots, X_n$ of law $\mathbb{P}_{\theta}$. The general idea is 
        \begin{enumerate}[left=10pt]
            \item find functions h,g such that we have the relation 
            \[\theta = h\left(E\left(g\left(X\right)\right)\right)\]
            with $X \sim \mathbb{P}_{\theta}$; 
            \item use the empirical mean $\frac{1}{n}\sum_{ i=1 } ^{ n }g\left(X_i\right)$ to estimate $E\left(g\left(X\right)\right)$; 
            \item use the estimator of $\theta$ given by 
            \[\hat{\theta}_n = h\left(\frac{1}{n}\sum_{ i=1 } ^{ n }g\left(X_i\right)\right).\]
        \end{enumerate}
        \begin{remark}{Example}
            \begin{tcolorbox}[gris]
                We can look at the case of $\mathbb{P}_{\theta}=\text{Geo}\left(p\right)\left(\text{ so that } \theta =p, \Theta = \left[0,1\right]\right)$.

                We know that if $X \sim \text{Geo}\left(p\right)$, 
                \[E\left(X\right)=\frac{1}{p}, \mathspace E\left(X^2\right)=\frac{2-p}{p^2}.\]
                Each of these give rise to estimator via the moment method.
                \begin{itemize}[left=10pt, label=\textbullet]
                    \item First consider the first moment. We can use $h\left(x\right)=x^{-1}$ and $g\left(x\right)=x$. This gives the estimator of p 
                    \[\hat{p}_n\left(X_1, \ldots, X_n\right)=\frac{n}{\sum_{ i=1 } ^{ n }X_i}.\]
                    
                    \item Then consider the second moment. We can use $h\left(x\right)=\frac{\sqrt{1+8x}-1}{2x}$ and $g\left(x\right)=x^2$.

                        This gives the estimator of p 
                        \[\hat{p}_n\left(X_1, \ldots, X_n\right)=\frac{n}{2 \sum_{ i=1 } ^{ n }X_i^2}\left(\left(1+\frac{8}{n}\sum_{ i=1 } ^{ n }X_i^2\right)^{\frac{1}{2}}-1\right).\]
                        
                \end{itemize}
            \end{tcolorbox}
        \end{remark}
    \subsection{Maximum likelihood estimator}
        This estimator is deeply linked to the Bayes formula: imagine that you have a realisation $x_1, \ldots, x_n$ of a sample $X_1, \ldots, X_n$ of law $\mathbb{P}_{\theta}$. The idea is to look for the value of $\theta$ which maximises the probability to observe the realisation $x_1, \ldots, x_n$.
        \begin{definition}
            Let $n >=1$, and a sample $X_1, \ldots, X_n$ of law $\mathbb{P}_{\theta}$. For a realisation $x_1, \ldots, x_n$ of $X_1, \ldots, X_n$, the \important{likelihood} of $x_1, \ldots, x_n$ given $\theta, \mathspace\mathcal{L}\left(x_1, \ldots, x_n | \theta\right)$, is defined by 
            \begin{itemize}[left=10pt, label=\textbullet]
                \item if $\mathbb{P}_{\theta}$ is a discrete probability measure (i.e.: the $X_i$'s are discrete random variables), 
                    \[\mathcal{L}\left(x_1, \ldots, x_n|\theta\right)=P\left(X_1=x_1, \ldots, X_n = x_n\right)= \prod_{i=1}^{ n } P\left(X_i =x_i\right)=\prod_{i=1}^{ n } \mathbb{P}_{\theta}\left(\{x_i\}\right)\]
                as the $X_i$'s are independent and of law $\mathbb{P}_{\theta}$; 
                \item if $\mathbb{P}_{\theta}$ is a continuous probability measure (i.e.: the $X_i$'s are continuous random variables), 
                \[\mathcal{L}\left(x_1, \ldots, x_n|\theta\right)=\prod_{i=1}^{ n } f_{X_i}\left(x_i\right)=\prod_{i=1}^{ n } f_{\theta}\left(x_i\right)\]
                where $f_{\theta}$ is the density function associated to $\mathbb{P}_{\theta}$.
            \end{itemize}
        \end{definition}
        \begin{definition}
            The \important{maximum likelihood estimator} of $\theta$ is given by 
        \[MLE\left(X_1, \ldots, X_n\right) =argmax_{\theta}\mathcal{L}\left(X_1, \ldots, X_n|\theta\right),\]
            the point $\theta$ at which $\mathcal{L}\left(X_1, \ldots, X_n|\theta\right)$ is maximized.
        \end{definition}
        \begin{remark}{Example}
            \begin{tcolorbox}[gris]
                Consider the case $X_1, \ldots, X_n$ is a n-sample of law $\mathcal{N}\left(\mu, \sigma ^2\right)$ (and take $\theta=\left(\mu, \sigma ^2\right)$). The likelihood function of a realisation $x_1, \ldots, x_n \in \mathbb{N}^*$ is given by 
                \[\mathcal{L}\left(x_1, \ldots, x_n| \mu, \sigma ^2\right) = \prod_{i=1}^{ n } \frac{e^{-\left(x_i-\mu\right)^2 \slash 2\sigma ^2}}{\sqrt{2\pi\sigma ^2}}.\]
                Maximizing $\mathcal{L}$ is equivalent to minimizing $-\ln\left(\mathcal{L}\right)$. 
                \[-\mathcal{L}\left(x_1, \ldots, x_n | \mu, \sigma ^2\right) =\frac{n}{2}\ln\left(2\pi\sigma ^2\right) + \sum_{ i =1  } ^{ n }\frac{\left(x_i - \mu\right)^2}{2\sigma ^2}.\]
                For any fixed $\sigma ^2$, the minima of this expression is reached at the empirical mean: $\mu_* = \frac{1}{n}\sum_{ i=1 } ^{ n }x_i$ (which is found by finding the critical ponts as a function of $\mu$). Then, the minima is reached at the empirical variance: $\sigma ^2 =\frac{1}{n}\sum_{ i=1 } ^{ n }\left(x_i - \mu_* \right)^2$ (which is again found by finding the critical points as a function of $\sigmag$). The maximum likelihood estimator of $\left(\mu, \sigma ^2\right)$ is thus given by 
                \[MLE\left(X_1, \ldots, X_n\right)=\left(\bar{X}_n, \hat{\sigma}_n^2\right).\]
            \end{tcolorbox}
        \end{remark}
\section{Quantile tables}
    In the topic of the next section, we will often be confronted to problem of this form: 
    \[P\left(X \leq t\right)=0.95,\]
    To avoid doing computations every time, people have computed thes values and made quantile tables.\
    \begin{definition}
        Let X be a random variables, $q > 0$ be an integer. For integer $k\geq1, t \in \mathbb{R}$ is a \important{kth q-quantile} of X if 
        \[P\left(X <t\right) \leq \frac{k}{q} \text{ AND } P\left(X\leqy\right)\geq\frac{k}{q}.\]
        Of X is a continuous random variabl with strictly positive density, there is only one kth q-quantile of X. Sometimes, one speaks about v-quantiles with v $\in \left(0,1\right)$. In this case, a kth v-quantile of X is a number $t \in \mathbb{R}$ such that 
        \[P\left(X <t\right) \leq kv \text{ AND } P\left(X \leq t\right) \geq kv.\]
    \end{definition}
    \begin{remark}{Example}
        \begin{tcolorbox}[gris]
            The 10-quantiles of the $\mathcal{N}\left(0,1\right)$ distribution are given in the next table. See the graph after for an illustration of the concept. In the table, $t_k$ is the number such that $P\left(X \leq t\right)=\frac{k}{10}$ where $ X \sim \mathcal{N}\left(0,1\right)$.
        \end{tcolorbox}
        \begin{center}
        \includegraphics[width=10cm]{images/3.png}
        \includegraphics[width=8cm]{images/4.png}
        \end{center}
    \end{remark}
\section{Confidence intervals}
    Confidence intervals will allow to state things such as ``given the measurements $X_1, \ldots, X_n$, the parameter $\theta$ belongs to the interval $I\left(X_1, \ldots, X_n\right)$ with probability $p\left(X_1, \ldots, X_n\right)$'', where I, p, are to be sepcified. One is typically interested in finding intervals with very short length (precise estimation) with p close to 1 (small probability of error).
    \begin{definition}
        Let $X_1, \ldots, X_n$ be a sample of law $\mathbb{P}_{\theta}$. Let $\alpha \in \left(0,1\right)$. A ranndom interval $ I = I\left(X_1, \ldots, X_n\right)$ note depending on $\theta$ is called a \important{level $1- \alpha$ confidence interval fo $f\left(\theta\right)$} if for every $\theta \in \Theta$, 
        \[P\left(f\left(\theta\right) \in I \left(X_1, \ldots, X_n\right)\right)=1-\alpha.\]
        $1-\alpha$ is called the \important{confidence level} of the estimation.
    \end{definition}
    \begin{definition}
        A confidence interval $I = I \left(X_1, \ldots, X_n\right)$ is an \important{excess confidence interval for $f\left(\theta\right)$ at level $1 -\alpha$} if 
        \[P\left(f\left(\theta\right) \in I \left(X_1, \ldots, X_n\right)\right) \geq 1- \alpha.\]
        
    \end{definition}
    
    
    
    
    
        
        
        
        
        
        
             
        
            
         
            
        
        

